{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training\n",
    "## Author: [Jeremiah Croshaw](https://linktr.ee/jeremiahcroshaw)\n",
    "#### Last Edited: Sept 23 2020\n",
    "\n",
    "Since this code was written while employed by [Quantum Silicon Inc.](https://www.quantumsilicon.com/), I have been advised to share it under the GNU-GPL\n",
    "***\n",
    "Copyright (C) 2020  Jeremiah Croshaw\n",
    "\n",
    "This program is free software; you can redistribute it and/or\n",
    "modify it under the terms of the GNU General Public License\n",
    "as published by the Free Software Foundation; version 2\n",
    "of the License.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program; if not, write to the Free Software\n",
    "Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n",
    "***\n",
    "\n",
    "### This code demonstrates how the neural network is trained from prepared data.\n",
    "### The NN architecture corresponds to the highest performing NN as shown in Croshaw's PhD Thesis\n",
    "\n",
    "The network design was done by Croshaw, but some of the code infrastructure was developed with Rashidi for the linked work (below)\n",
    "\n",
    "### This code was developed for follow up work to our [published work](https://iopscience.iop.org/article/10.1088/2632-2153/ab6d5e) on defect segmentation of scanning probe images of the H-Si(100) surface.  \n",
    "\n",
    "\n",
    "author corresponence: croshaw@ualberta.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\crosh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import keras\n",
    "import keras.backend as K \n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dice_coef():\n",
    "***\n",
    "input:\n",
    "- y_true - ground truths (labelled data)\n",
    "- y_pred - predicted by NN\n",
    "- smooth - smoothing factor\n",
    "\n",
    "output:\n",
    "- returns the calculated dice_coef\n",
    "\n",
    "### dice_coef_loss():\n",
    "***\n",
    "input - same as above\n",
    "\n",
    "output - negative dice_coef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred,smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_seg():\n",
    "used to define the neural network\n",
    "\n",
    "inputs:\n",
    "- pretrained_weights - to be used if you want to initialize the netowrk with pretrained weights\n",
    "- input size - (None,None, 1).  This allows any size image to be used as input (resolution is the only restriction)\n",
    "- dropout_rate - add if you want any drop out\n",
    "- kernel_size_small - useful if you want to use the same kernel size throughout the network\n",
    "- kernel_size_large - different kernel sizes are useful to explore\n",
    "- padding = 'same'\n",
    "- activation = 'relu'\n",
    "- pool_size = (2,2)):\n",
    "\n",
    "output:\n",
    "- model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_seg(pretrained_weights = None,\n",
    "                 input_size = (None,None,1),\n",
    "               dropout_rate = 0.0,\n",
    "                kernel_size_small = (3,3),\n",
    "                    kernel_size_large = (5,5),\n",
    "                    padding = 'same',\n",
    "                 activation = 'relu',\n",
    "                  pool_size = (2,2)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(filters = 64, kernel_size=kernel_size_small, padding=padding)(inputs)\n",
    "    drop1 = Dropout(dropout_rate)(conv1)\n",
    "    norm1 = BatchNormalization()(drop1)\n",
    "    act1  = Activation(activation)(norm1)\n",
    "\n",
    "    conv1b = Conv2D(filters = 64, kernel_size=kernel_size_small, padding=padding)(act1)\n",
    "    drop1b = Dropout(dropout_rate)(conv1b)\n",
    "    norm1b = BatchNormalization()(drop1b)\n",
    "    act1b  = Activation(activation)(norm1b)\n",
    "\n",
    "    pool1 = MaxPool2D()(act1b)\n",
    "\n",
    "    conv2 = Conv2D(filters = 128, kernel_size=kernel_size_small, padding=padding)(pool1)\n",
    "    drop2 = Dropout(dropout_rate)(conv2)\n",
    "    norm2 = BatchNormalization()(drop2)\n",
    "    act2  = Activation(activation)(norm2)\n",
    "\n",
    "    conv2b = Conv2D(filters = 128, kernel_size=kernel_size_small, padding=padding)(act2)\n",
    "    drop2b = Dropout(dropout_rate)(conv2b)\n",
    "    norm2b = BatchNormalization()(drop2b)\n",
    "    act2b  = Activation(activation)(norm2b)\n",
    "    \n",
    "    pool2 = MaxPool2D()(act2b)\n",
    "\n",
    "    conv3 = Conv2D(filters = 256 , kernel_size=kernel_size_small, padding=padding)(pool2)\n",
    "    drop3 = Dropout(dropout_rate)(conv3)\n",
    "    norm3 = BatchNormalization()(drop3)\n",
    "    act3  = Activation(activation)(norm3)\n",
    "\n",
    "    conv3b = Conv2D(filters = 256 , kernel_size=kernel_size_small, padding=padding)(act3)\n",
    "    drop3b = Dropout(dropout_rate)(conv3b)\n",
    "    norm3b = BatchNormalization()(drop3b)\n",
    "    act3b  = Activation(activation)(norm3b)\n",
    "    \n",
    "    pool3 = MaxPool2D()(act3b)\n",
    "\n",
    "   \n",
    "    up8   = UpSampling2D()(pool3)\n",
    "    drop8 = Dropout(dropout_rate)(up8)\n",
    "    conv8 = Conv2D(filters = 256, kernel_size=kernel_size_small, padding=padding)(drop8)\n",
    "    norm8 = BatchNormalization()(conv8)\n",
    "    act8 = Activation(activation)(norm8)\n",
    "\n",
    "    drop8b = Dropout(dropout_rate)(act8)\n",
    "    conv8b = Conv2D(filters = 256, kernel_size=kernel_size_small, padding=padding)(drop8b)\n",
    "    norm8b = BatchNormalization()(conv8b)\n",
    "    act8b  = Activation(activation)(norm8b)\n",
    "\n",
    "    up9   = UpSampling2D()(act8b)\n",
    "    drop9 = Dropout(dropout_rate)(up9)\n",
    "    conv9 = Conv2D(filters = 128, kernel_size=kernel_size_small, padding=padding)(drop9)\n",
    "    norm9 = BatchNormalization()(conv9)\n",
    "    act9 = Activation(activation)(norm9)\n",
    "\n",
    "    drop9b = Dropout(dropout_rate)(act9)\n",
    "    conv9b = Conv2D(filters = 128, kernel_size=kernel_size_small, padding=padding)(drop9b)\n",
    "    norm9b = BatchNormalization()(conv9b)    \n",
    "    act9b  = Activation(activation)(norm9b)\n",
    "\n",
    "    up10  = UpSampling2D()(act9b)\n",
    "    drop10 = Dropout(dropout_rate)(up10)\n",
    "    conv10 = Conv2D(filters = 64, kernel_size=kernel_size_small, padding=padding)(drop10)\n",
    "    norm10 = BatchNormalization()(conv10)\n",
    "    act10 = Activation(activation)(norm10)\n",
    "    \n",
    "    drop10b = Dropout(dropout_rate)(act10)\n",
    "    conv10b = Conv2D(filters = 64, kernel_size=kernel_size_small, padding=padding)(drop10b)\n",
    "    norm10b = BatchNormalization()(conv10b)    \n",
    "    act10b  = Activation(activation)(norm10b)\n",
    "\n",
    "    conv11 = Conv2D(filters = 14, kernel_size=(3,3),padding='same')(act10b)\n",
    "    resh11 = Reshape((-1,14))(conv11)\n",
    "    act11  = Activation('softmax')(resh11)\n",
    "\n",
    "    model = Model(inputs = [inputs],outputs = [act11])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n",
    "***\n",
    "imports the trained data (this data should be randomized previously to biases when using mini_batch)\n",
    "creates the model, then trains.\n",
    "\n",
    "The model checkpoints are saved to a csv file so they can be plotted when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('.\\\\data_file.h5','r')\n",
    "\n",
    "#data should already be divided into train, test, and validate to keep this constant throughout training\n",
    "#Here X corresponds to images, y corresponds to labels\n",
    "X_train=h5f['X_train']  \n",
    "y_train=h5f['y_train'] \n",
    "y_train=np.reshape(y_train,(-1,16384,14)) # reshape for one hot encoding, middle number corresponds to size of data set\n",
    "X_test=h5f['X_test']\n",
    "y_test=h5f['y_test']\n",
    "y_test=np.reshape(y_test,(-1,16384,14))\n",
    "X_val=h5f['X_val']\n",
    "y_val=h5f['y_val']\n",
    "y_val=np.reshape(y_val,(-1,16384,14))\n",
    "h5f.close\n",
    "\n",
    "# print the shape of the tain, test, valsidaion data set\n",
    "print(\n",
    "\"X_train shape:\", X_train.shape,'\\n',\n",
    "\"y_train shape:\", y_train.shape,'\\n',\n",
    "\"X_test shape:\", X_test.shape,'\\n',\n",
    "\"y_test shape:\", y_test.shape, '\\n',\n",
    "\"X_val shape:\", X_val.shape,'\\n',\n",
    "\"y_val shape:\", y_val.shape,'\\n'\n",
    ")\n",
    "\n",
    "#creat the model\n",
    "model=model_seg(0)\n",
    "\n",
    "#define the optimizer\n",
    "optimizer = Adam() # parameters for Adam() can be tuned for training\n",
    "\n",
    "#in case a learning rate decay should be used\n",
    "def lr_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 1\n",
    "    lrate = initial_lrate * math.pow(drop,epoch)\n",
    "    return lrate\n",
    "\n",
    "################################################################\n",
    "#define some checkpoint saves to monitor how training is going\n",
    "model_name = \"model_8c_256_double-double-conv\"\n",
    "lrRate_callback=keras.callbacks.LearningRateScheduler(lr_decay, verbose=1)\n",
    "model_checkpoint=keras.callbacks.ModelCheckpoint(\"./checkpoints_model_8c/weights_256_.{epoch:02d}-{val_loss:.2f}-{val_dice_coef:.2f}.hdf5\", \n",
    "                 monitor='loss', verbose=1, save_best_only=True,save_weights_only=False, mode='min', period=5)\n",
    "csv_callback=keras.callbacks.CSVLogger('./checkpoints_model_8c/{}.csv'.format(model_name), separator=',', append=False)\n",
    "callbacks=[lrRate_callback,model_checkpoint,csv_callback]\n",
    "################################################################\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy',dice_coef])\n",
    "model.summary()\n",
    "model_name = \"model_8c\"\n",
    "#run this instead if you want to load a previously trained model\n",
    "#model=load_model('.\\\\previously_trained_model.hdf5',custom_objects={'dice_coef': dice_coef,'dice_coef_loss':dice_coef_loss})\n",
    "\n",
    "#define training epochs\n",
    "epochs = 100\n",
    "\n",
    "#start the training process\n",
    "model.fit(x=X_train,y=y_train,epochs=epochs, batch_size=50,validation_split=0,validation_data=(X_val,y_val),\n",
    "          callbacks=callbacks,shuffle=\"batch\")\n",
    "\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training\n",
    "***\n",
    "if you are interested in seeing how the training is going, this code will plot based on the .csv files saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# define the learning rate function that is used for training in order to plot it as a function number of epochs\n",
    "\n",
    "def lr_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 1\n",
    "    lrate = initial_lrate * math.pow(drop,epoch)\n",
    "    return lrate\n",
    "\n",
    "#read in the desired CSV file\n",
    "df=pd.read_csv('./checkpoints_model_8/model_8_128_double-conv.csv')   \n",
    "df['lr']=df['epoch'].apply(lr_decay) \n",
    "\n",
    "    \n",
    "fig,ax=plt.subplots(2,3,figsize=(15,6))\n",
    "    \n",
    "ax[0][0].scatter(df['epoch'],df['loss'],color='blue')\n",
    "ax[0][0].plot(df['epoch'],df['loss'],color='blue')\n",
    "ax[0][0].set_xlabel('epoch')\n",
    "ax[0][0].set_ylabel('loss')\n",
    "    \n",
    "ax[0][1].scatter(df['epoch'],df['dice_coef'],color='red')\n",
    "ax[0][1].plot(df['epoch'],df['dice_coef'],color='red')\n",
    "ax[0][1].set_xlabel('epoch')\n",
    "ax[0][1].set_ylabel('dice_coef')\n",
    "    \n",
    "ax[0][2].scatter(df['epoch'],df['lr'],color='green')\n",
    "ax[0][2].plot(df['epoch'],df['lr'],color='green')\n",
    "ax[0][2].set_xlabel('epoch')\n",
    "ax[0][2].set_ylabel('learning rate')\n",
    "ax[0][2].set_yscale('log')\n",
    "    \n",
    "ax[1][0].scatter(df['epoch'],df['val_loss'],color='blue')\n",
    "ax[1][0].plot(df['epoch'],df['val_loss'],color='blue')\n",
    "ax[1][0].set_xlabel('epoch')\n",
    "ax[1][0].set_ylabel('val_loss')\n",
    "    \n",
    "ax[1][1].scatter(df['epoch'],df['val_dice_coef'],color='red')\n",
    "ax[1][1].plot(df['epoch'],df['val_dice_coef'],color='red')\n",
    "ax[1][1].set_xlabel('epoch')\n",
    "ax[1][1].set_ylabel('val_dice_coef')\n",
    "    \n",
    "ax[1][2].scatter(df['epoch'],df['val_acc'],color='green')\n",
    "ax[1][2].plot(df['epoch'],df['val_acc'],color='green')\n",
    "ax[1][2].set_xlabel('epoch')\n",
    "ax[1][2].set_ylabel('val_acc')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3,hspace = 0.3)\n",
    "\n",
    "#plt.show()\n",
    "fig.savefig('./checkpoints_model_8/training_plots.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
